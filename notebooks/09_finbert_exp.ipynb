{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # to avoid warnings\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys \n",
    "\n",
    "\"\"\"\n",
    "Sklearn Libraries\n",
    "\"\"\"\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\"\"\"\n",
    "Transformer Libraries\n",
    "\"\"\"\n",
    "from transformers import BertTokenizer,  AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "\"\"\"\n",
    "Pytorch Libraries\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "\n",
    "sys.path.append('../')\n",
    "from src.utilities.config_ import train_data_path\n",
    "import src.utilities.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842</th>\n",
       "      <td>2</td>\n",
       "      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4843</th>\n",
       "      <td>1</td>\n",
       "      <td>Rinkuskiai 's beer sales fell by 6.5 per cent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844</th>\n",
       "      <td>2</td>\n",
       "      <td>Operating profit fell to EUR 35.4 mn from EUR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4845</th>\n",
       "      <td>2</td>\n",
       "      <td>Net sales of the Paper segment decreased to EU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4846</th>\n",
       "      <td>2</td>\n",
       "      <td>Sales in Finland decreased by 10.5 % in Januar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4846 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "1         1  According to Gran , the company has no plans t...\n",
       "2         1  Technopolis plans to develop in stages an area...\n",
       "3         2  The international electronic industry company ...\n",
       "4         0  With the new production plant the company woul...\n",
       "5         0  According to the company 's updated strategy f...\n",
       "...     ...                                                ...\n",
       "4842      2  LONDON MarketWatch -- Share prices ended lower...\n",
       "4843      1  Rinkuskiai 's beer sales fell by 6.5 per cent ...\n",
       "4844      2  Operating profit fell to EUR 35.4 mn from EUR ...\n",
       "4845      2  Net sales of the Paper segment decreased to EU...\n",
       "4846      2  Sales in Finland decreased by 10.5 % in Januar...\n",
       "\n",
       "[4846 rows x 2 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv\n",
    "data = pd.read_csv(os.path.join(train_data_path, \"finance-dataset.csv\"),\n",
    "                   encoding='latin-1', \n",
    "                    names=['label', 'text']).iloc[1:]\n",
    "\n",
    "# Convert labels to integers\n",
    "label_to_int = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "data['label'] = data['label'].map(label_to_int)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "# Load the FinBERT model\n",
    "model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4846/4846 [00:01<00:00, 2597.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert pandas DataFrame to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(data)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=64)  # You can adjust the max_length as needed\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Rename the label column to \"labels\" for the trainer\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# Remove unnecessary columns\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "\n",
    "# Convert to torch tensors\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments with evaluation steps\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=4,  # Set a high number of epochs to allow for early stopping\n",
    "    per_device_train_batch_size=8,  # Adjust based on your memory\n",
    "    per_device_eval_batch_size=16,  # Adjust based on your memory\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,  # Log every 100 steps\n",
    "    evaluation_strategy=\"steps\",  # Evaluate every 'eval_steps'\n",
    "    eval_steps=500,  # Evaluation interval, adjust based on your dataset size\n",
    "    log_level=\"error\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4521, 'learning_rate': 1e-05, 'epoch': 0.21}\n",
      "{'loss': 0.7295, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.5691, 'learning_rate': 3e-05, 'epoch': 0.62}\n",
      "{'loss': 0.6397, 'learning_rate': 4e-05, 'epoch': 0.82}\n",
      "{'loss': 0.5958, 'learning_rate': 5e-05, 'epoch': 1.03}\n",
      "{'eval_loss': 0.5578722357749939, 'eval_accuracy': 0.8072164948453608, 'eval_runtime': 50.225, 'eval_samples_per_second': 19.313, 'eval_steps_per_second': 1.215, 'epoch': 1.03}\n",
      "{'loss': 0.4561, 'learning_rate': 4.652777777777778e-05, 'epoch': 1.24}\n",
      "{'loss': 0.4593, 'learning_rate': 4.305555555555556e-05, 'epoch': 1.44}\n",
      "{'loss': 0.4638, 'learning_rate': 3.958333333333333e-05, 'epoch': 1.65}\n",
      "{'loss': 0.417, 'learning_rate': 3.611111111111111e-05, 'epoch': 1.86}\n",
      "{'loss': 0.3437, 'learning_rate': 3.263888888888889e-05, 'epoch': 2.06}\n",
      "{'eval_loss': 0.7420837879180908, 'eval_accuracy': 0.8309278350515464, 'eval_runtime': 40.4004, 'eval_samples_per_second': 24.01, 'eval_steps_per_second': 1.51, 'epoch': 2.06}\n",
      "{'loss': 0.1994, 'learning_rate': 2.916666666666667e-05, 'epoch': 2.27}\n",
      "{'loss': 0.2438, 'learning_rate': 2.5694444444444445e-05, 'epoch': 2.47}\n",
      "{'loss': 0.1776, 'learning_rate': 2.2222222222222223e-05, 'epoch': 2.68}\n",
      "{'loss': 0.2188, 'learning_rate': 1.8750000000000002e-05, 'epoch': 2.89}\n",
      "{'loss': 0.1197, 'learning_rate': 1.527777777777778e-05, 'epoch': 3.09}\n",
      "{'eval_loss': 0.8685851693153381, 'eval_accuracy': 0.8525773195876288, 'eval_runtime': 39.8108, 'eval_samples_per_second': 24.365, 'eval_steps_per_second': 1.532, 'epoch': 3.09}\n",
      "Stopping training as eval_accuracy has reached 0.8525773195876288\n",
      "{'train_runtime': 2168.8505, 'train_samples_per_second': 7.148, 'train_steps_per_second': 0.894, 'train_loss': 0.6723511295318604, 'epoch': 3.09}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=0.6723511295318604, metrics={'train_runtime': 2168.8505, 'train_samples_per_second': 7.148, 'train_steps_per_second': 0.894, 'train_loss': 0.6723511295318604, 'epoch': 3.09})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainerCallback, TrainerState, TrainerControl\n",
    "import numpy as np\n",
    "\n",
    "class EarlyStoppingCallback(TrainerCallback):\n",
    "    def __init__(self, threshold: float, metric: str = \"eval_accuracy\"):\n",
    "        self.threshold = threshold\n",
    "        self.metric = metric\n",
    "\n",
    "    def on_evaluate(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        if state.log_history and self.metric in state.log_history[-1]:\n",
    "            accuracy = state.log_history[-1][self.metric]\n",
    "            if accuracy > self.threshold:\n",
    "                control.should_training_stop = True\n",
    "                print(f\"Stopping training as {self.metric} has reached {accuracy}\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# Define the custom callback with your threshold\n",
    "early_stopping_callback = EarlyStoppingCallback(threshold=0.85, metric=\"eval_accuracy\")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    callbacks=[early_stopping_callback],\n",
    "    compute_metrics=compute_metrics,  # Define a compute_metrics function to calculate accuracy\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8525773195876288, 'f1': 0.852897273228443, 'precision': 0.8533935043857552, 'recall': 0.8525773195876288}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Define the metric function\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = compute_metrics(predictions)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 970\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative', 'positive']\n"
     ]
    }
   ],
   "source": [
    "# Your new text\n",
    "new_text = [\"This is a sample financial news article. The market is looking bad.\", \"Market is looking great now\"]\n",
    "\n",
    "# Tokenize the new text\n",
    "inputs = tokenizer(new_text, padding='max_length', truncation=True, max_length=64, return_tensors=\"pt\")\n",
    "\n",
    "# Move inputs to the appropriate device (e.g., CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Map predictions to labels\n",
    "int_to_label = {0: 'positive', 1: 'neutral', 2: 'negative'}\n",
    "predicted_labels = [int_to_label[pred.item()] for pred in predictions]\n",
    "\n",
    "print(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../model/finbert/tokenizer_config.json',\n",
       " '../model/finbert/special_tokens_map.json',\n",
       " '../model/finbert/vocab.txt',\n",
       " '../model/finbert/added_tokens.json')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to save the model and tokenizer\n",
    "save_directory = \"../model/finbert/\"\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the saved directory\n",
    "save_directory = \"../model/finbert/\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "# Load the model\n",
    "model = BertForSequenceClassification.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dummy training arguments (only `per_device_eval_batch_size` is relevant here)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_eval_batch_size=16,  # Adjust based on your memory\n",
    ")\n",
    "\n",
    "# Create Trainer instance with the loaded model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "# Predict on the test dataset\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8525773195876288\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.80      0.80       271\n",
      "     neutral       0.89      0.88      0.88       583\n",
      "    positive       0.81      0.84      0.83       116\n",
      "\n",
      "    accuracy                           0.85       970\n",
      "   macro avg       0.83      0.84      0.84       970\n",
      "weighted avg       0.85      0.85      0.85       970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "# Extract the true labels\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(true_labels, preds, target_names=['negative', 'neutral', 'positive'])\n",
    "accuracy = accuracy_score(true_labels, preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "market-dashboard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
