{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # to avoid warnings\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys \n",
    "\n",
    "\"\"\"\n",
    "Sklearn Libraries\n",
    "\"\"\"\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\"\"\"\n",
    "Transformer Libraries\n",
    "\"\"\"\n",
    "from transformers import BertTokenizer,  AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "\"\"\"\n",
    "Pytorch Libraries\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "\n",
    "sys.path.append('../')\n",
    "from src.utilities.config_ import train_data_path\n",
    "import src.utilities.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842</th>\n",
       "      <td>2</td>\n",
       "      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4843</th>\n",
       "      <td>1</td>\n",
       "      <td>Rinkuskiai 's beer sales fell by 6.5 per cent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844</th>\n",
       "      <td>2</td>\n",
       "      <td>Operating profit fell to EUR 35.4 mn from EUR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4845</th>\n",
       "      <td>2</td>\n",
       "      <td>Net sales of the Paper segment decreased to EU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4846</th>\n",
       "      <td>2</td>\n",
       "      <td>Sales in Finland decreased by 10.5 % in Januar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4846 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "1         1  According to Gran , the company has no plans t...\n",
       "2         1  Technopolis plans to develop in stages an area...\n",
       "3         2  The international electronic industry company ...\n",
       "4         0  With the new production plant the company woul...\n",
       "5         0  According to the company 's updated strategy f...\n",
       "...     ...                                                ...\n",
       "4842      2  LONDON MarketWatch -- Share prices ended lower...\n",
       "4843      1  Rinkuskiai 's beer sales fell by 6.5 per cent ...\n",
       "4844      2  Operating profit fell to EUR 35.4 mn from EUR ...\n",
       "4845      2  Net sales of the Paper segment decreased to EU...\n",
       "4846      2  Sales in Finland decreased by 10.5 % in Januar...\n",
       "\n",
       "[4846 rows x 2 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv\n",
    "data = pd.read_csv(os.path.join(train_data_path, \"finance-dataset.csv\"),\n",
    "                   encoding='latin-1', \n",
    "                    names=['label', 'text']).iloc[1:]\n",
    "\n",
    "# Convert labels to integers\n",
    "label_to_int = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "data['label'] = data['label'].map(label_to_int)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "# Load the FinBERT model\n",
    "model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4846/4846 [00:01<00:00, 4042.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert pandas DataFrame to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(data)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=64)  # You can adjust the max_length as needed\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Rename the label column to \"labels\" for the trainer\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# Remove unnecessary columns\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "\n",
    "# Convert to torch tensors\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments with adjusted logging\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,  # Adjust based on your memory\n",
    "    per_device_eval_batch_size=16,  # Adjust based on your memory\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_strategy=\"steps\",  # Adjust logging strategy\n",
    "    logging_steps=100,  # Log every 100 steps to reduce logging output\n",
    "    log_level=\"error\",  # Set log level to \"error\" to reduce output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2558, 'learning_rate': 1e-05, 'epoch': 0.21}\n",
      "{'loss': 0.7412, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.6889, 'learning_rate': 3e-05, 'epoch': 0.62}\n",
      "{'loss': 0.6228, 'learning_rate': 4e-05, 'epoch': 0.82}\n",
      "{'loss': 0.553, 'learning_rate': 5e-05, 'epoch': 1.03}\n",
      "{'loss': 0.4394, 'learning_rate': 4.4764397905759164e-05, 'epoch': 1.24}\n",
      "{'loss': 0.4286, 'learning_rate': 3.9528795811518326e-05, 'epoch': 1.44}\n",
      "{'loss': 0.3984, 'learning_rate': 3.429319371727749e-05, 'epoch': 1.65}\n",
      "{'loss': 0.3786, 'learning_rate': 2.905759162303665e-05, 'epoch': 1.86}\n",
      "{'loss': 0.2846, 'learning_rate': 2.382198952879581e-05, 'epoch': 2.06}\n",
      "{'loss': 0.1559, 'learning_rate': 1.8586387434554976e-05, 'epoch': 2.27}\n",
      "{'loss': 0.1146, 'learning_rate': 1.3350785340314136e-05, 'epoch': 2.47}\n",
      "{'loss': 0.1215, 'learning_rate': 8.115183246073298e-06, 'epoch': 2.68}\n",
      "{'loss': 0.1271, 'learning_rate': 2.879581151832461e-06, 'epoch': 2.89}\n",
      "{'train_runtime': 1734.1295, 'train_samples_per_second': 6.705, 'train_steps_per_second': 0.839, 'train_loss': 0.6447465542665461, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1455, training_loss=0.6447465542665461, metrics={'train_runtime': 1734.1295, 'train_samples_per_second': 6.705, 'train_steps_per_second': 0.839, 'train_loss': 0.6447465542665461, 'epoch': 3.0})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainerCallback, TrainerState, TrainerControl\n",
    "\n",
    "class EarlyStoppingCallback(TrainerCallback):\n",
    "    def __init__(self, threshold: float):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def on_step_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        if state.log_history and 'loss' in state.log_history[-1]:\n",
    "            loss = state.log_history[-1]['loss']\n",
    "            if loss < self.threshold:\n",
    "                control.should_training_stop = True\n",
    "\n",
    "# Define the custom callback with your threshold\n",
    "early_stopping_callback = EarlyStoppingCallback(threshold=0.1)\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8422680412371134, 'f1': 0.8416675393188416, 'precision': 0.8412930836752625, 'recall': 0.8422680412371134}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Define the metric function\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = compute_metrics(predictions)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 970\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative', 'positive']\n"
     ]
    }
   ],
   "source": [
    "# Your new text\n",
    "new_text = [\"This is a sample financial news article. The market is looking bad.\", \"Market is looking great now\"]\n",
    "\n",
    "# Tokenize the new text\n",
    "inputs = tokenizer(new_text, padding='max_length', truncation=True, max_length=64, return_tensors=\"pt\")\n",
    "\n",
    "# Move inputs to the appropriate device (e.g., CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Map predictions to labels\n",
    "int_to_label = {0: 'positive', 1: 'neutral', 2: 'negative'}\n",
    "predicted_labels = [int_to_label[pred.item()] for pred in predictions]\n",
    "\n",
    "print(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../model/finbert/tokenizer_config.json',\n",
       " '../model/finbert/special_tokens_map.json',\n",
       " '../model/finbert/vocab.txt',\n",
       " '../model/finbert/added_tokens.json')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to save the model and tokenizer\n",
    "save_directory = \"../model/finbert/\"\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the saved directory\n",
    "save_directory = \"../model/finbert/\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "# Load the model\n",
    "model = BertForSequenceClassification.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dummy training arguments (only `per_device_eval_batch_size` is relevant here)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_eval_batch_size=16,  # Adjust based on your memory\n",
    ")\n",
    "\n",
    "# Create Trainer instance with the loaded model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "# Predict on the test dataset\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8546391752577319\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.77      0.78       277\n",
      "     neutral       0.89      0.90      0.89       577\n",
      "    positive       0.83      0.84      0.83       116\n",
      "\n",
      "    accuracy                           0.85       970\n",
      "   macro avg       0.84      0.83      0.84       970\n",
      "weighted avg       0.85      0.85      0.85       970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "# Extract the true labels\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(true_labels, preds, target_names=['negative', 'neutral', 'positive'])\n",
    "accuracy = accuracy_score(true_labels, preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "market-dashboard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
